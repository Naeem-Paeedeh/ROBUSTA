experiment_description = "Phase 3,Inc. learning"
phase = "incremental_learning"
device = "cuda:0"
seed = 2
batch_size_base = 200
batch_size_test = 100
batch_size_new = 0
image_size = 224
in_channels = 3
tqdm_enabled = true
resume = false


[configs_arch]
  model_type = 'CCT-14/7x2'
  use_BatchNorm = true
  use_BatchNorm_for_patch_embeddings = true
  temperature_stochastic_classifier = 16.0
  temperature_cosine_classifier = 10.0      # Initial value of Tau in eq. (1) of the "Prototype Rectification for Few-Shot Learning" paper.
  PositionalEmbeddingType = 'Learnable'     # SinCos
  dropout_rate_classifier_head = 0.0
  number_of_the_first_layers_to_be_frozen = 0
  classifer_head_type = 'Stochastic'        # 'Stochastic', 'Cosine', or 'Linear'


[configs_dataset]
  dataroot = "/scratch/gx83/np9254/Datasets/FSCIL/CEC/"
  dataset_name = "mini_imagenet"   # 'mini_imagenet', 'cifar100', or 'cub200'
  num_workers = 10
  total_classes = 100
  num_base_classes = 60
  num_tasks = 9
  num_shots = 5
  drop_last_base = true


[configs_FSCIL]
  num_epochs = [4, 15, 15, 15, 15, 15, 15, 15, 15]
  update_mu = true
  fine_tune = true
  freeze_backbone = true
  use_delta_parameters_for_base_task = true
  use_prefixes_for_distance_calculations = true
  use_shared_covariance = true
  start_from_task = 0
  randomize_selected_classes = false
  tasks_or_classes_for_Mahalanobis_distance_calculations = "classes"  # "classes" or "tasks"
  enable_Mahalanobis_distance = true                                  # We use Euclidean distance when it is false
  use_pseudo_labeled_samples_for_task_identification = [true, true, true, true, true, true, true, true, true]

  [configs_FSCIL.configs_PEFT]
    prefix_seq_length = [16, 16, 16, 16, 16, 16, 16, 16, 16]
    number_of_layers_for_prefixes = [-1, -1, -1, -1, -1, -1, -1, -1, -1] # -1 to use prefixes in all blocks
    fusion_mode = 'last'                # last or mean or random
    prefix_or_prompt = 'prefix'         # 'prefix' or 'prompt'

  [configs_FSCIL.optimizer]
    optimizer_name = 'AdamW' # SGD, Adam, AdamW, or Rprop.
    lr_head = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
    lr_prefixes_or_prompts = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
    lr = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
    lr_backbone = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
    momentum = 0.9
    momentum2 = 0.999   # For Adam
    dampening = 0
    nesterov = true     # For SGD
    weight_decay = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

  [configs_FSCIL.scheduler]
    name = "ReduceLROnPlateau"
    mode = "min"
    factor = 0.25
    patience = 5
    cooldown = 0
    min_lr = 0
    verbose = true
    moving_average_capacity = 10
  
  [configs_FSCIL.evaluation]
    ignore_logits_for_other_tasks = true
    stochastic = true

  [configs_FSCIL.PredictionNet]
    enabled = false


[configs_logger]
  display_interval = 0.5
  display_freq = 50
  moving_average_capacity = 50


[configs_save]
  save_freq_epoch = 10
  save_freq_iter = 2000
  time_interval_to_save = 60
  root = "/scratch/gx83/np9254/ROBUSTA-Saves/Mini-ImageNet/Ablation_studies/without_prediction_net"
  input_file = "/scratch/gx83/np9254/ROBUSTA-Saves/Mini-ImageNet/Phase_2,60_classes/P1P2,start_time=Date_2024-01-21,Time_10-03-18,seed=1-Best_Model.pt"
  output_file = "P1P2P3"
