experiment_description = "Incremental learning,100 base classes"
phase = "incremental_learning"
device = "cuda:0"
seed = 43
batch_size_base = 160
batch_size_test = 160
batch_size_new = 0
image_size = 224
in_channels = 3
tqdm_enabled = true
resume = false


[configs_arch]
  model_type = 'CCT-21/7x2'
  use_BatchNorm = true
  use_BatchNorm_for_patch_embeddings = true
  temperature_stochastic_classifier = 16.0
  temperature_cosine_classifier = 10.0      # Initial value of Tau in eq. (1) of the "Prototype Rectification for Few-Shot Learning" paper.
  PositionalEmbeddingType = 'Learnable'     # Learnable, SinCos
  dropout_rate_classifier_head = 0.0
  number_of_the_first_layers_to_be_frozen = 0
  classifer_head_type = 'Stochastic'        # 'Stochastic', 'Cosine', 'Linear'


[configs_dataset]
  dataroot = "/scratch/gx83/np9254/Datasets/FSCIL/CEC/"
  dataset_name = 'cub200'                   # 'mini_imagenet', 'cifar100', or 'cub200'
  num_workers = 16
  total_classes = 200
  num_base_classes = 100
  num_tasks = 11
  num_shots = 5
  drop_last_base = true


[configs_FSCIL]
  num_epochs = [5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
  update_mu = true
  fine_tune = true
  freeze_backbone = true
  use_delta_parameters_for_base_task = false
  use_prefixes_for_distance_calculations = true
  use_shared_covariance = true
  start_from_task = 0
  randomize_selected_classes = false
  tasks_or_classes_for_Mahalanobis_distance_calculations = "classes"  # "classes" or "tasks"
  enable_Mahalanobis_distance = true                                  # We use Euclidean distance when it is false
  use_pseudo_labeled_samples_for_task_identification = [true, true, true, true, true, true, true, true, true, true, true]

  [configs_FSCIL.configs_PEFT]
    prefix_seq_length = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
    number_of_layers_for_prefixes = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] # -1 to use prefixes in all blocks
    fusion_mode = 'last'                # last, mean, random, zeros
    prefix_or_prompt = 'prefix'         # 'prefix' or 'prompt'

  [configs_FSCIL.optimizer]
    optimizer_name = 'AdamW'            # SGD, Adam, AdamW, or Rprop.
    lr_head = [1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2]
    lr_prefixes_or_prompts = [1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2]
    lr = [1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2]
    lr_backbone = [1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2]        # If it is not frozen!
    momentum = 0.9
    momentum2 = 0.999   # For Adam
    dampening = 0
    nesterov = true     # For SGD
    weight_decay = [1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5]

  [configs_FSCIL.scheduler]
    name = "ReduceLROnPlateau"
    mode = "min"
    factor = 0.25
    patience = 5
    cooldown = 0
    min_lr = 0
    verbose = true
    moving_average_capacity = 10
  
  [configs_FSCIL.evaluation]
    ignore_logits_for_other_tasks = true
    stochastic = true

  [configs_FSCIL.PredictionNet]
    enabled = true
    num_epochs = [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]
    separate_PredictionNet_for_each_task = true
    use_PredictionNet_for_this_task = [true, true, true, true, true, true, true, true, true, true, true]
    use_pseudo_labeled_test_samples = [false, true, true, true, true, true, true, true, true, true, true]
    batch_size_for_Pseudo_labelling = 100
    batch_size_for_PredictionNet = 100
    n_layers = 1
    size_hidden_layer = 384
    use_real_residual_connections = false
    dropout_rate = 0.0
    bias = true
    num_outliers = [8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    display_freq = 50
    remember_from_previous_task = true    # Remember the PredictionNet parameters from the previous task when we use separate RestorNets.
    use_the_best_model = false    # With minimum loss
    loss = "MSE"                  # "squared_Euclidean_distance" or "MSE"

    [configs_FSCIL.PredictionNet.optimizer]
      optimizer_name = 'AdamW' # SGD, Adam, AdamW, or Rprop.
      lr = [1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3]   # For Adam
      momentum = 0.9
      momentum2 = 0.999   # For Adam
      dampening = 0.0
      nesterov = true     # For SGD
      weight_decay = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

    [configs_FSCIL.PredictionNet.scheduler]
      name = "ReduceLROnPlateau"
      mode = "min"
      factor = 0.25
      patience = 1000
      cooldown = 0
      min_lr = 0
      verbose = true
      moving_average_capacity = 10


[configs_logger]
  display_interval = 0.5
  display_freq = 50
  moving_average_capacity = 50


[configs_save]
  save_freq_epoch = 10
  save_freq_iter = 2000
  time_interval_to_save = 60
  root = "/scratch/gx83/np9254/ROBUSTA-Saves/CUB-200/Phase_3,100_classes/5-shot"
  input_file = "/scratch/gx83/np9254/ROBUSTA-Saves/CUB-200/Phase_2,100_classes/P0P1P2,start_time=Date_2024-01-02,Time_09-50-53,seed=41-Best_Model.pt"
  output_file = "P0P1P2P3"
